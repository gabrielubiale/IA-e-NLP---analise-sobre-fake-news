{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7rNASgXLkN/fMBbvwk3w/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabrielubiale/IA-e-NLP---analise-sobre-fake-news/blob/main/FakeNews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from bs4 import BeautifulSoup\n",
        "import re,string,unicodedata\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from string import punctuation\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "h6y0XC88gu19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"allBase.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "MrOhEpmQL690"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Criar o gráfico de contagem\n",
        "sns.countplot(x=\"category\", data=df)\n",
        "\n",
        "# Configurar os rótulos dos eixos\n",
        "plt.xlabel(\"0 = true 1 = fake\")\n",
        "plt.ylabel(\"Contagem\")\n",
        "\n",
        "# Exibir o gráfico\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Yt0kQoTwL3hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stop words são palavras comuns e frequentes em um idioma que geralmente são removidas durante o processamento de texto, como em tarefas de análise de texto, processamento de linguagem natural e recuperação de informações. Essas palavras não fornecem informações distintivas sobre o conteúdo do texto e podem ser consideradas \"ruído\" em muitos casos.\n",
        "\n",
        "As stop words geralmente incluem pronomes, preposições, conjunções e outras palavras muito comuns que não contribuem significativamente para o significado geral de um texto. Alguns exemplos de stop words em inglês são \"a\", \"an\", \"the\", \"in\", \"on\", \"is\", \"and\", \"but\", entre outras.\n",
        "\n",
        "A remoção de stop words é uma etapa comum no pré-processamento de texto para reduzir a dimensionalidade do texto e melhorar a eficiência e a qualidade de análises subsequentes, como análise de sentimentos, classificação de textos e modelagem de tópicos. A ideia por trás da remoção de stop words é se concentrar nas palavras-chave mais significativas e relevantes em um texto, que podem fornecer insights valiosos para a análise ou modelagem em questão."
      ],
      "metadata": {
        "id": "TkWe9m_Sc_Zn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "# Baixar os stopwords em inglês\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Criar um conjunto de stopwords em inglês\n",
        "stop = set(stopwords.words('english'))\n",
        "\n",
        "# Criar uma lista de pontuações\n",
        "# é uma string contendo todos os caracteres de pontuação.\n",
        "punctuation = list(string.punctuation)\n",
        "\n",
        "# Adicionar as pontuações ao conjunto de stopwords\n",
        "stop.update(punctuation)\n",
        "print(stop)\n"
      ],
      "metadata": {
        "id": "8O8JgQkkhlI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def strip_html(text):\n",
        "    \"\"\"\n",
        "    Remove tags HTML do texto utilizando a biblioteca BeautifulSoup.\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "def remove_between_square_brackets(text):\n",
        "    \"\"\"\n",
        "    Remove o conteúdo entre colchetes quadrados do texto utilizando expressões regulares.\n",
        "    \"\"\"\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "\n",
        "def remove_urls(text):\n",
        "    \"\"\"\n",
        "    Remove URLs do texto utilizando expressões regulares.\n",
        "    \"\"\"\n",
        "    return re.sub(r'http\\S+', '', text)\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    \"\"\"\n",
        "    Remove as stopwords do texto.\n",
        "    \"\"\"\n",
        "    final_text = []\n",
        "    for i in text.split():\n",
        "        if i.strip().lower() not in stop:\n",
        "            final_text.append(i.strip())\n",
        "    return \" \".join(final_text)\n",
        "\n",
        "def denoise_text(text):\n",
        "    \"\"\"\n",
        "    Aplica uma sequência de etapas de pré-processamento para remover ruídos do texto.\n",
        "    \"\"\"\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    text = remove_stopwords(text)\n",
        "    return text\n",
        "\n",
        "# Aplica as funções na coluna text\n",
        "df['text'] = df['text'].apply(denoise_text)\n",
        "\n",
        "\n",
        "#MELHORIAS:\n",
        "#rodar todo o codigo novamente\n",
        "#limpeza de dados"
      ],
      "metadata": {
        "id": "yLJquCBthm9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Até esse momento houveram algumas configurações e normalização"
      ],
      "metadata": {
        "id": "es4KcGjod3Cu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cria uma nuvem de palavras com o novo data set para os dados fake\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "# Define o tamanho da figura\n",
        "plt.figure(figsize=(20, 20))\n",
        "\n",
        "# Cria a nuvem de palavras\n",
        "wc = WordCloud(max_words=10000, width=1600, height=800, stopwords=STOPWORDS).generate(\" \".join(df[df.category == 1].text))\n",
        "\n",
        "# Exibe a imagem da nuvem de palavras\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "\n",
        "# Mostra o gráfico\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gN78nNHShnmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cria uma nuvem de palavras com o novo data set para os dados true\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "plt.figure(figsize=(20, 20))  # Define o tamanho da figura do gráfico\n",
        "\n",
        "# Cria a nuvem de palavras\n",
        "wc = WordCloud(max_words=2000, width=1600, height=800, stopwords=STOPWORDS).generate(\" \".join(df[df.category == 0].text))\n",
        "\n",
        "plt.imshow(wc, interpolation='bilinear')  # Exibe a imagem da nuvem de palavras\n",
        "\n",
        "plt.show()  # Exibe o gráfico"
      ],
      "metadata": {
        "id": "BGzouxG7hov5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
        "# Cria uma figura com dois subplots lado a lado, com uma largura de 12 unidades e altura de 8 unidades.\n",
        "\n",
        "text_len = df[df['category'] == 1]['text'].str.len()\n",
        "ax1.hist(text_len, color='red')\n",
        "# Obtém o comprimento dos textos na coluna 'text' onde a categoria é igual a 1.\n",
        "# Plota um histograma dos comprimentos dos textos no primeiro subplot (ax1), usando a cor vermelha.\n",
        "\n",
        "ax1.set_title('Comprimento dos textos fake')\n",
        "# Define o título do primeiro subplot como 'Original text'.\n",
        "\n",
        "text_len = df[df['category'] == 0]['text'].str.len()\n",
        "ax2.hist(text_len, color='green')\n",
        "# Obtém o comprimento dos textos na coluna 'text' onde a categoria é igual a 0.\n",
        "# Plota um histograma dos comprimentos dos textos no segundo subplot (ax2), usando a cor verde.\n",
        "\n",
        "ax2.set_title('Comprimento dos textos verdadeiros')\n",
        "# Define o título do segundo subplot como 'Fake text'.\n",
        "\n",
        "fig.suptitle('Número de caracteres no texto')\n",
        "# Define o título da figura principal como 'Characters in texts'.\n",
        "\n",
        "plt.show()\n",
        "# Exibe a figura com os subplots.\n",
        "\n",
        "\n",
        "#esses subplots mostram a distribuição dos comprimentos dos textos nas categorias 0 e 1.\n",
        "#Compara-se os dois plots para ver diferença de comprimento\n",
        "#entre os textos verdadeiros e falsos"
      ],
      "metadata": {
        "id": "gqWHnVS3hpwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar uma figura com dois subplots lado a lado\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
        "\n",
        "# Calcular o número de palavras nos textos da categoria 1 (verdadeiros) e criar um histograma\n",
        "text_len = df[df['category'] == 1]['text'].str.split().map(lambda x: len(x))\n",
        "ax1.hist(text_len, color='red')\n",
        "ax1.set_title('Texto falso')\n",
        "\n",
        "# Calcular o número de palavras nos textos da categoria 0 (falsos) e criar um histograma\n",
        "text_len = df[df['category'] == 0]['text'].str.split().map(lambda x: len(x))\n",
        "ax2.hist(text_len, color='green')\n",
        "ax2.set_title('Texto verdadeiro')\n",
        "\n",
        "# título do gráfico\n",
        "fig.suptitle('Comparação entre o numero de palavras dos textos falsos e verdadeiros')\n",
        "\n",
        "# Exibir a figura\n",
        "plt.show()\n",
        "\n",
        "# mostram a distribuição do número de palavras nos textos"
      ],
      "metadata": {
        "id": "mNQjIigIhq5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criação da figura e dos dois subplots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
        "\n",
        "# Análise dos textos originais (categoria 1)\n",
        "word = df[df['category'] == 1]['text'].str.split().apply(lambda x: [len(i) for i in x])\n",
        "sns.distplot(word.map(lambda x: np.mean(x)), ax=ax1, color='red')\n",
        "ax1.set_title('dados falsos')\n",
        "\n",
        "# Análise dos textos falsos (categoria 0)\n",
        "word = df[df['category'] == 0]['text'].str.split().apply(lambda x: [len(i) for i in x])\n",
        "sns.distplot(word.map(lambda x: np.mean(x)), ax=ax2, color='green')\n",
        "ax2.set_title('dados verdadeiros')\n",
        "\n",
        "# Configuração do título principal da figura\n",
        "fig.suptitle('Comprimento médio das palavras por categoria')\n",
        "\n",
        "#Mostram a distribuição do comprimento médio das palavras nos textos das categorias\n",
        "#tamanho médio das palavras por texto\n",
        "\n",
        "\n",
        "#MELHORIA:\n",
        "\n",
        "#analisar melhor o gráfico e fazer a curva padrão"
      ],
      "metadata": {
        "id": "3YgHxyGXhr9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_corpus(text):\n",
        "    # Cria uma lista vazia para armazenar as palavras\n",
        "    words = []\n",
        "\n",
        "    # Percorre cada texto no parâmetro 'text'\n",
        "    for i in text:\n",
        "        # Divide o texto em palavras usando o método split()\n",
        "        for j in i.split():\n",
        "            # Remove espaços em branco no início e no final de cada palavra e adiciona à lista 'words'\n",
        "            words.append(j.strip())\n",
        "\n",
        "    # Retorna a lista de palavras\n",
        "    return words\n",
        "\n",
        "# Chama a função 'get_corpus' passando o texto do DataFrame 'df' como argumento e armazena o resultado na variável 'corpus'\n",
        "corpus = get_corpus(df.text)\n",
        "\n",
        "#corpus é uma lista que começa no zero e vai até o 20° elemento\n",
        "corpus[:21]\n",
        "\n",
        "#\"corpus\" se refere a uma lista de palavras extraídas dos textos presentes na coluna 'text' do DataFrame df.\n",
        "#A função get_corpus é usada para percorrer cada texto dividir em palavras usando o método split(), remover\n",
        "#espaços em branco em cada palavra e adicioná-las na lista\n",
        "#Corpus é uma lista que contém todas as palavras presentes nos textos.\n",
        "#fornece uma coleção de palavras que podem ser utilizadas para diversas finalidades"
      ],
      "metadata": {
        "id": "SXPbQb9OhtiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_corpus(text):\n",
        "    words = []\n",
        "\n",
        "    for i in text:\n",
        "        for j in i.split():\n",
        "            words.append(j.strip())\n",
        "    return words\n",
        "\n",
        "category_1_text = df[df['category'] == 1]['text']\n",
        "\n",
        "corpus = get_corpus(category_1_text)\n",
        "\n",
        "corpus[:20] #corpus é uma lista que começa no zero e vai até o 20° elemento\n"
      ],
      "metadata": {
        "id": "kY3ARhkhjw62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Cria um objeto Counter para contar a ocorrência de cada elemento no corpus\n",
        "counter = Counter(corpus)\n",
        "\n",
        "# Obtém as 10 palavras mais comuns do corpus\n",
        "most_common = counter.most_common(20)\n",
        "\n",
        "# Converte a lista de tuplas em um dicionário para facilitar o acesso aos dados\n",
        "most_common = dict(most_common)\n",
        "\n",
        "# Imprime o dicionário com as palavras mais comuns e suas contagens\n",
        "most_common\n"
      ],
      "metadata": {
        "id": "ICz_ghQZhzAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def get_top_text_ngrams(corpus, n, g):\n",
        "    \"\"\"\n",
        "    Retorna os n-gramas mais frequentes em um corpus de texto.\n",
        "\n",
        "    Parâmetros:\n",
        "        - corpus: lista de documentos de texto.\n",
        "        - n: número de n-gramas mais frequentes a serem retornados.\n",
        "        - g: tamanho dos n-gramas (1 para unigramas, 2 para bigramas, etc.).\n",
        "\n",
        "    Retorna:\n",
        "        - Lista de tuplas contendo o n-grama e sua contagem de ocorrências, ordenada por frequência decrescente.\n",
        "    \"\"\"\n",
        "\n",
        "    # Inicializa o objeto CountVectorizer com o tamanho dos n-gramas\n",
        "    vec = CountVectorizer(ngram_range=(g, g)).fit(corpus)\n",
        "\n",
        "    # Transforma o corpus em uma matriz esparsa de contagens de n-gramas\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "\n",
        "    # Calcula a soma das contagens de cada n-grama\n",
        "    sum_words = bag_of_words.sum(axis=0)\n",
        "\n",
        "    # Cria uma lista de tuplas contendo o n-grama e sua contagem\n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "\n",
        "    # Ordena a lista de tuplas por frequência decrescente\n",
        "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Retorna os n-gramas mais frequentes\n",
        "    return words_freq[:n]\n",
        "\n",
        "\n",
        "#A função retorna uma lista de tuplas contendo os n-gramas mais frequentes, ordenados por frequência decrescente\n",
        "#N-gramas são sequências de palavras em um texto. Eles nos ajudam a entender a estrutura e o significado desse texto."
      ],
      "metadata": {
        "id": "vuxDUaf-k6s2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define o tamanho da figura do gráfico\n",
        "plt.figure(figsize=(16, 9))\n",
        "\n",
        "# Obtém os unigramas mais comuns a partir da coluna 'text' do DataFrame 'df'\n",
        "most_common_uni = get_top_text_ngrams(df.text, 20, 1)\n",
        "\n",
        "# Converte o resultado em um dicionário\n",
        "most_common_uni = dict(most_common_uni)\n",
        "\n",
        "# Cria um gráfico de barras utilizando o seaborn\n",
        "# O eixo x contém os valores dos unigramas mais comuns (contagem) e o eixo y contém as chaves (palavras)\n",
        "sns.barplot(x=list(most_common_uni.values()), y=list(most_common_uni.keys()))\n",
        "\n",
        "#visualizar os n-gramas mais comuns em um texto.\n",
        "#visualizar a sequencia de palavras mais comuns"
      ],
      "metadata": {
        "id": "j5KM-X9Ph02s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define o tamanho da figura do gráfico\n",
        "plt.figure(figsize=(16, 9))\n",
        "\n",
        "# Obtém os 10 bigramas mais comuns no texto\n",
        "most_common_bi = get_top_text_ngrams(df[df['category'] == 1]['text'], 20, 2)\n",
        "\n",
        "# Converte o resultado para um dicionário\n",
        "most_common_bi = dict(most_common_bi)\n",
        "\n",
        "# Cria um gráfico de barras com os bigramas mais comuns\n",
        "sns.barplot(x=list(most_common_bi.values()), y=list(most_common_bi.keys()))\n",
        "\n",
        "# Exibe o gráfico\n",
        "plt.show()\n",
        "\n",
        "#gráfico de barras que mostra os bigramas mais frequentes em um texto,\n",
        "#permitindo uma visualização combinações de duas palavras mais comuns"
      ],
      "metadata": {
        "id": "l8TqzED9h1-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Definir o tamanho da figura\n",
        "plt.figure(figsize=(16, 9))\n",
        "\n",
        "# Obter os top 10 trigramas mais comuns do texto\n",
        "most_common_tri = get_top_text_ngrams(df.text, 20, 3)\n",
        "\n",
        "# Converter o resultado em um dicionário\n",
        "most_common_tri = dict(most_common_tri)\n",
        "\n",
        "# Criar o gráfico de barras utilizando o Seaborn\n",
        "sns.barplot(x=list(most_common_tri.values()), y=list(most_common_tri.keys()))\n",
        "\n",
        "# Exibir o gráfico\n",
        "plt.show()\n",
        "\n",
        "#trigramas"
      ],
      "metadata": {
        "id": "e-pY_fE3h29M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}